{"cells":[{"cell_type":"markdown","metadata":{"id":"peoAdSrTDqP2"},"source":["# <h1 align= 'center'>PyTorch RNN Modeling</h1>"]},{"cell_type":"markdown","metadata":{"id":"daqdd2lyQepI"},"source":["The goal for this analysis is to predict if a review rates the movie positively or negatively.\n","<a href=\"https://imgur.com/FfdEBRz\"><img src=\"https://i.imgur.com/FfdEBRzm.png\" title=\"source: imgur.com\" align=\"right\"></a>\n","- IMDB movie reviews dataset\n","- http://ai.stanford.edu/~amaas/data/sentiment\n","- Contains 25000 positive and 25000 negative reviews\n","- Contains at most reviews per movie\n","- At least 7 stars out of 10 $\\rightarrow$ positive (label = 1)\n","- At most 4 stars out of 10 $\\rightarrow$ negative (label = 0)\n","\n","> **Here, we use PyTorch to train and test recurrent neural network to classify sentiments of reviews by inputting word embeddings to an LSTM layer which is connected to a fully connected layer**"]},{"cell_type":"markdown","metadata":{"id":"a3NKRSykDS0G"},"source":["## <h2> <center>Dependencies</center></h2>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dqbcy5rGukws","outputId":"f300a446-6c6a-4092-cb2f-6d448eb9b08c","executionInfo":{"status":"ok","timestamp":1684654932854,"user_tz":-330,"elapsed":6710,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import re\n","import nltk\n","import torch\n","import unicodedata\n","import numpy as np\n","import pandas as pd\n","nltk.download('stopwords')\n","from bs4 import BeautifulSoup\n","from collections import Counter\n","from sklearn import preprocessing\n","from nltk.stem import SnowballStemmer\n","from torch.utils.data import TensorDataset, DataLoader"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"radvvFiCupOC","outputId":"4cc5b77d-257a-4c1c-8ea0-e50654e77ff3","scrolled":true,"executionInfo":{"status":"ok","timestamp":1684655170681,"user_tz":-330,"elapsed":5727,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                  review sentiment\n","33467  I went to see Ashura as 2005 Fantasia Festival...  positive\n","18094  An underrated addition to the Graham Greene ci...  positive\n","47594  I love Tudor Chirila and maybe that's why i en...  positive\n","11059  I agree with most of the critics above. More y...  negative\n","5964   Contrary to most other commentators, I deeply ...  negative\n","14996  And it's not because since her days on \"Claris...  positive\n","7266   Farrah Fawcett gives an award nominated perfor...  positive"],"text/html":["\n","  <div id=\"df-040b84b1-38f4-4ab4-ab0b-b588599dc40c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33467</th>\n","      <td>I went to see Ashura as 2005 Fantasia Festival...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>18094</th>\n","      <td>An underrated addition to the Graham Greene ci...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>47594</th>\n","      <td>I love Tudor Chirila and maybe that's why i en...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>11059</th>\n","      <td>I agree with most of the critics above. More y...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>5964</th>\n","      <td>Contrary to most other commentators, I deeply ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>14996</th>\n","      <td>And it's not because since her days on \"Claris...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>7266</th>\n","      <td>Farrah Fawcett gives an award nominated perfor...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-040b84b1-38f4-4ab4-ab0b-b588599dc40c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-040b84b1-38f4-4ab4-ab0b-b588599dc40c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-040b84b1-38f4-4ab4-ab0b-b588599dc40c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["from google.colab import drive \n","drive.mount('/content/drive')\n","movies = pd.read_csv('/content/drive/MyDrive/imdb_data.csv')\n","movies.sample(7)"]},{"cell_type":"markdown","metadata":{"id":"C3E69t4ODRke"},"source":["## <h2> <center>Preprocessing</center></h2>"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3z8MMjITupdS","outputId":"337cf12a-c9af-48eb-f415-68ca5623a290","executionInfo":{"status":"ok","timestamp":1684655175711,"user_tz":-330,"elapsed":1084,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review  sentiment\n","0  One of the other reviewers has mentioned that ...          1\n","1  A wonderful little production. <br /><br />The...          1\n","2  I thought this was a wonderful way to spend ti...          1\n","3  Basically there's a family where a little boy ...          0\n","4  Petter Mattei's \"Love in the Time of Money\" is...          1"],"text/html":["\n","  <div id=\"df-77317661-b32b-45ad-afce-a428d18edc89\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77317661-b32b-45ad-afce-a428d18edc89')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-77317661-b32b-45ad-afce-a428d18edc89 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-77317661-b32b-45ad-afce-a428d18edc89');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["# Cateogrize positive and negative as 1 and 0 respectively\n","label_encoder = preprocessing.LabelEncoder()\n","movies['sentiment'] = label_encoder.fit_transform(movies['sentiment'])\n","movies.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"8a_SfAx2upkR","executionInfo":{"status":"ok","timestamp":1684655186977,"user_tz":-330,"elapsed":816,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["def review_to_wordlist(review, custom = True, stem_words = True):\n","    # Clean the text, with the option to remove stopwords and stem words.\n","    \n","    # Strip html\n","    soup = BeautifulSoup(review, \"html.parser\")\n","    [s.extract() for s in soup(['iframe', 'script'])]\n","    stripped_text = soup.get_text()\n","    review_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n","    \n","    # replace accents\n","    review_text = unicodedata.normalize('NFKD', review_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    review_text = re.sub(r\"[^A-Za-z0-9!?\\'\\`]\", \" \", review_text) # remove special characters\n","    review_text = contractions.fix(review_text) # expand contractions\n","    \n","    review_text = review_text.lower()\n","    words = review_text.split()\n","    if custom:\n","        stop_words = set(nltk.corpus.stopwords.words('english'))\n","        stop_words.update(['movie', 'film', 'one', 'would', 'even', \n","                           'movies', 'films', 'cinema',\n","                           'character', 'show', \"'\", \"!\", 'like'])\n","    else:\n","        stop_words = set(nltk.corpus.stopwords.words('english'))\n","    words = [w for w in words if not w in stop_words]\n","    review_text = \" \".join(words)\n","        \n","    review_text = re.sub(r\"!\", \" ! \", review_text)\n","    review_text = re.sub(r\"\\?\", \" ? \", review_text)\n","    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n","    \n","    if stem_words:\n","        words = review_text.split()\n","        stemmer = SnowballStemmer('english')\n","        stemmed_words = [stemmer.stem(word) for word in words]\n","        review_text = \" \".join(stemmed_words)\n","    \n","    # Return a list of words, with each word as its own string\n","    return review_text"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABBzydjp6cMI","outputId":"dd054efe-27dd-4edd-ab37-9c7b53c43f34","executionInfo":{"status":"ok","timestamp":1684655198055,"user_tz":-330,"elapsed":6883,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"]}],"source":["!pip install contractions\n","import contractions"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cVOZLyIupie","executionInfo":{"status":"ok","timestamp":1684655314689,"user_tz":-330,"elapsed":113284,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}},"outputId":"31e0c52d-3d0e-4a76-9cc2-bbde67b28a9b"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-9a8991df8b77>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(review, \"html.parser\")\n"]}],"source":["movies['review'] = movies['review'].apply(review_to_wordlist)"]},{"cell_type":"markdown","metadata":{"id":"GYU_JNupDY8p"},"source":["## <h2> <center>Features</center></h2>"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ZEuuJ2A0upgo","executionInfo":{"status":"ok","timestamp":1684655333946,"user_tz":-330,"elapsed":746,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["all_text = ''\n","all_text = '\\n'.join([review for review in movies['review']])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FD_FWts_y0t","outputId":"345b2e08-4f9a-48af-9fe6-8ea8500637ff","executionInfo":{"status":"ok","timestamp":1684655335894,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["34732415"]},"metadata":{},"execution_count":13}],"source":["len(all_text)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYNAqctrupbf","outputId":"082223ab-3f3f-445b-f17c-bddd4dc4b380","executionInfo":{"status":"ok","timestamp":1684655339309,"user_tz":-330,"elapsed":557,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5723713"]},"metadata":{},"execution_count":14}],"source":["# split by new lines and spaces\n","reviews_split = all_text.split('\\n')\n","all_text = ' '.join(reviews_split)\n","\n","# create a list of words\n","words = all_text.split()\n","len(words)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7CVUaM6wupZJ","executionInfo":{"status":"ok","timestamp":1684655343452,"user_tz":-330,"elapsed":817,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["counts = Counter(words)\n","vocab = sorted(counts, key = counts.get, reverse = True)\n","word_id = {word: i for i, word in enumerate(vocab, 1)} # dictionary to words to ints\n","\n","review_ids = []\n","for review in reviews_split:\n","    review_ids.append([word_id[word] for word in review.split()])"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ByAvYWz_Rhp","outputId":"830c8c12-7e4f-4f2d-fea7-88e420890de7","executionInfo":{"status":"ok","timestamp":1684655345391,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{},"execution_count":16}],"source":["len(review_ids) # list of lists"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9W9VvMRAhFq","outputId":"2db65536-d319-42f8-8479-179745788e0d","executionInfo":{"status":"ok","timestamp":1684655348631,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["50000"]},"metadata":{},"execution_count":17}],"source":["encoded_labels = np.array(movies['sentiment'])\n","len(encoded_labels)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGSp7xmUupXg","outputId":"49a54fcf-e2f6-4aee-e7f6-fe56eab71238","executionInfo":{"status":"ok","timestamp":1684655351017,"user_tz":-330,"elapsed":2,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words:  74429\n","\n"]}],"source":["# stats about vocabulary\n","print('Unique words: ', len((word_id))) \n","print()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d227QiIiupVN","outputId":"588064b8-3c2e-44cb-9829-105448001778","executionInfo":{"status":"ok","timestamp":1684655353636,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum review length: 1437\n"]}],"source":["# outlier review stats\n","review_lens = Counter([len(x) for x in review_ids])\n","print(\"Maximum review length: {}\".format(max(review_lens)))"]},{"cell_type":"markdown","metadata":{"id":"xgQIbJf6-VKS"},"source":["> **Padding the sequences**"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"ETmNIB_3-MjJ","executionInfo":{"status":"ok","timestamp":1684655357197,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["def pad_features(review_ids, seq_length):\n","    \"\"\" Return features of review_ints, where each review is \n","        padded with 0's or truncated to the input seq_length.\n","    \"\"\" \n","\n","    features = np.zeros((len(review_ids), seq_length), dtype=int)\n","\n","    # for each review, truncate the review to seq_length\n","    for i, row in enumerate(review_ids):\n","        features[i, -len(row):] = np.array(row)[:seq_length]\n","    \n","    return features"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaHFIeI--Mp5","outputId":"0628d258-3bdb-4809-d6f7-dbe2d30041b0","executionInfo":{"status":"ok","timestamp":1684655360364,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [  112   116  3343    34    39    76 12258   202  3343   464]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [ 1880  3517   765  5681  1006  2343   203  1284   362    22]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [  185   414   301   416   141 10263    39     3   503   336]]\n"]}],"source":["seq_length = 200\n","\n","features = pad_features(review_ids, seq_length=seq_length)\n","\n","## test statements - do not change - ##\n","assert len(features)==len(review_ids), \"Your features should have as many rows as reviews.\"\n","assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n","\n","# print first 10 values of the first 30 batches \n","print(features[:30,:10])"]},{"cell_type":"markdown","metadata":{"id":"l--BvnlMChJJ"},"source":["## <h2> <center>Split the data</center></h2>"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2eIVSU5CekL","outputId":"7fda513b-da1c-4a2c-9035-dc10950daa3d","executionInfo":{"status":"ok","timestamp":1684655981774,"user_tz":-330,"elapsed":983,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\t\t\tFeature Shapes:\n","Train set: \t\t(40000, 200) \n","Validation set: \t(5000, 200) \n","Test set: \t\t(5000, 200)\n"]}],"source":["split_frac = 0.8\n","\n","## split data into training, validation, and test data (features and labels, x and y)\n","split_idx = int(len(features)*0.8)\n","train_x, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","\n","## print out the shapes of your resultant feature data\n","print(\"\\t\\t\\tFeature Shapes:\")\n","print(\"Train set: \\t\\t{}\".format(train_x.shape), \n","      \"\\nValidation set: \\t{}\".format(val_x.shape),\n","      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n"]},{"cell_type":"markdown","metadata":{"id":"tdOV2O0WCr_K"},"source":["> **Dataloaders**"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"_IJzYh2LCehf","executionInfo":{"status":"ok","timestamp":1684655368166,"user_tz":-330,"elapsed":2,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"-y2sMJFtCee8","executionInfo":{"status":"ok","timestamp":1684655381211,"user_tz":-330,"elapsed":675,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["# dataloaders\n","batch_size = 50\n","\n","# make sure to SHUFFLE your data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Df7enBiCebV","outputId":"7812a2aa-ac99-42c2-e262-a24d231080f6","executionInfo":{"status":"ok","timestamp":1684655385433,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample input size:  torch.Size([50, 200])\n","Sample input: \n"," tensor([[   0,    0,    0,  ...,  501,   10,  428],\n","        [   0,    0,    0,  ...,  313,  691,   67],\n","        [   0,    0,    0,  ...,  649,  550,  428],\n","        ...,\n","        [   0,    0,    0,  ...,  568,  647,  321],\n","        [   0,    0,    0,  ..., 1040,   12, 1513],\n","        [   0,    0,    0,  ..., 3252,  239,    1]])\n","\n","Sample label size:  torch.Size([50])\n","Sample label: \n"," tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n","        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n","        1, 0])\n"]}],"source":["dataiter = iter(train_loader)\n","for sample_x, sample_y in dataiter:\n","    print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","    print('Sample input: \\n', sample_x)\n","    print()\n","    print('Sample label size: ', sample_y.size()) # batch_size\n","    print('Sample label: \\n', sample_y)\n","    break\n"]},{"cell_type":"markdown","metadata":{"id":"fDL0UeboDIpE"},"source":["## <h2> <center>RNN with PyTorch</center></h2>\n","---\n","# Sentiment Network with PyTorch\n","\n","The layers are as follows:\n","1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n","2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n","3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n","4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_VSfzeQ-Mn2","outputId":"048ea7fb-94e5-48a8-deba-9a348faaa120","executionInfo":{"status":"ok","timestamp":1684655390070,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU.\n"]}],"source":["# First checking if GPU is available\n","train_on_gpu=torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"OormiUsFDAzw","executionInfo":{"status":"ok","timestamp":1684655393271,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class SentimentRNN(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super(SentimentRNN, self).__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear and sigmoid layers\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sig = nn.Sigmoid()\n","        \n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # embeddings and lstm_out\n","        embeds = self.embedding(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","    \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        # dropout and fully-connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        # sigmoid function\n","        sig_out = self.sig(out)\n","        \n","        # reshape to be batch_size first\n","        sig_out = sig_out.view(batch_size, -1)\n","        sig_out = sig_out[:, -1] # get last batch of labels\n","        \n","        # return last sigmoid output and hidden state\n","        return sig_out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n"]},{"cell_type":"markdown","metadata":{"id":"C0yhabNmEdY6"},"source":["> **Instantiate the network**\n","\n","* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n","* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n","* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n","* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n","* `n_layers`: Number of LSTM layers in the network. Typically between 1-3\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BR7wT78FDA3P","outputId":"e45cd955-956e-4074-c436-730f607bd143","executionInfo":{"status":"ok","timestamp":1684655402332,"user_tz":-330,"elapsed":905,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentRNN(\n","  (embedding): Embedding(74430, 400)\n","  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n","  (sig): Sigmoid()\n",")\n"]}],"source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(word_id)+1 # +1 for the 0 padding + our word tokens\n","output_size = 1\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 2\n","\n","net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"LP-SRowxEtgB"},"source":["> **Training**"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"3qHUQ1V5DAx5","executionInfo":{"status":"ok","timestamp":1684655407452,"user_tz":-330,"elapsed":935,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["# loss and optimization functions\n","lr= 1e-2\n","\n","criterion = nn.BCELoss() # binary classification\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_R0-4KjDAvM","outputId":"6fc8a34a-2074-406a-9835-169f5f90c3cf","executionInfo":{"status":"ok","timestamp":1684655620887,"user_tz":-330,"elapsed":209538,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/4... Step: 100... Loss: 0.314067... Val Loss: 0.452309\n","Epoch: 1/4... Step: 200... Loss: 0.502516... Val Loss: 0.402665\n","Epoch: 1/4... Step: 300... Loss: 0.359628... Val Loss: 0.359375\n","Epoch: 1/4... Step: 400... Loss: 0.355464... Val Loss: 0.347294\n","Epoch: 1/4... Step: 500... Loss: 0.667143... Val Loss: 0.340062\n","Epoch: 1/4... Step: 600... Loss: 0.297685... Val Loss: 0.343553\n","Epoch: 1/4... Step: 700... Loss: 0.338691... Val Loss: 0.365326\n","Epoch: 1/4... Step: 800... Loss: 0.402119... Val Loss: 0.340825\n","Epoch: 2/4... Step: 900... Loss: 0.207419... Val Loss: 0.329560\n","Epoch: 2/4... Step: 1000... Loss: 0.252502... Val Loss: 0.364728\n","Epoch: 2/4... Step: 1100... Loss: 0.320059... Val Loss: 0.337132\n","Epoch: 2/4... Step: 1200... Loss: 0.299975... Val Loss: 0.375009\n","Epoch: 2/4... Step: 1300... Loss: 0.349033... Val Loss: 0.432669\n","Epoch: 2/4... Step: 1400... Loss: 0.276925... Val Loss: 0.390780\n","Epoch: 2/4... Step: 1500... Loss: 0.460764... Val Loss: 0.345922\n","Epoch: 2/4... Step: 1600... Loss: 0.405221... Val Loss: 0.347863\n","Epoch: 3/4... Step: 1700... Loss: 0.405501... Val Loss: 0.396796\n","Epoch: 3/4... Step: 1800... Loss: 0.281865... Val Loss: 0.375722\n","Epoch: 3/4... Step: 1900... Loss: 0.139499... Val Loss: 0.363671\n","Epoch: 3/4... Step: 2000... Loss: 0.293782... Val Loss: 0.342636\n","Epoch: 3/4... Step: 2100... Loss: 0.205672... Val Loss: 0.346637\n","Epoch: 3/4... Step: 2200... Loss: 0.298805... Val Loss: 0.340360\n","Epoch: 3/4... Step: 2300... Loss: 0.458941... Val Loss: 0.344720\n","Epoch: 3/4... Step: 2400... Loss: 0.417887... Val Loss: 0.372745\n","Epoch: 4/4... Step: 2500... Loss: 0.264699... Val Loss: 0.359535\n","Epoch: 4/4... Step: 2600... Loss: 0.409888... Val Loss: 0.396806\n","Epoch: 4/4... Step: 2700... Loss: 0.145668... Val Loss: 0.382523\n","Epoch: 4/4... Step: 2800... Loss: 0.495365... Val Loss: 0.441463\n","Epoch: 4/4... Step: 2900... Loss: 0.558410... Val Loss: 0.530206\n","Epoch: 4/4... Step: 3000... Loss: 0.219475... Val Loss: 0.357079\n","Epoch: 4/4... Step: 3100... Loss: 0.246258... Val Loss: 0.424635\n","Epoch: 4/4... Step: 3200... Loss: 0.354992... Val Loss: 0.422580\n","Done.\n"]}],"source":["# training params\n","\n","epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","                if(train_on_gpu):\n","                    inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output.squeeze(), labels.float())\n","\n","                val_losses.append(val_loss.item())\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n","print(\"Done.\")"]},{"cell_type":"markdown","metadata":{"id":"qbiNs7krFEL4"},"source":["\n","> **Testing**"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"dFDrlpQJ-Ml3","executionInfo":{"status":"ok","timestamp":1684655640629,"user_tz":-330,"elapsed":1546,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    if(train_on_gpu):\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","    \n","    # get predicted outputs\n","    output, h = net(inputs, h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHmfIVgV-MgQ","outputId":"a9c83c9b-9056-4bb6-b021-0ab8f02a55c4","executionInfo":{"status":"ok","timestamp":1684655646550,"user_tz":-330,"elapsed":858,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.426\n","Test accuracy: 80.800\n"]}],"source":["\n","# -- stats! -- #\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc*100))"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"YiD5Y7cjFN9h","executionInfo":{"status":"ok","timestamp":1684655691510,"user_tz":-330,"elapsed":41957,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}}},"outputs":[],"source":["train_losses = []  # track loss\n","num_correct_train = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.train()  # set the model to training mode\n","# iterate over training data\n","for inputs, labels in train_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    if train_on_gpu:\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","\n","    # zero the gradients\n","    optimizer.zero_grad()\n","\n","    # get predicted outputs\n","    output, h = net(inputs, h)\n","\n","    # calculate loss\n","    loss = criterion(output.squeeze(), labels.float())\n","    train_losses.append(loss.item())\n","\n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","\n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct_train += np.sum(correct)\n","\n","    # perform backpropagation and optimization\n","    loss.backward()\n","    optimizer.step()\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vk4_YNJxQepj","executionInfo":{"status":"ok","timestamp":1684655696358,"user_tz":-330,"elapsed":958,"user":{"displayName":"Mohit Kapoor","userId":"05338043308895678403"}},"outputId":"c477adee-4cd3-48a8-bd6e-a975805973d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train loss: 0.439\n","Train accuracy: 79.517\n"]}],"source":["# -- stats! -- #\n","# avg train loss\n","print(\"Train loss: {:.3f}\".format(np.mean(train_losses)))\n","\n","# accuracy over all train data\n","train_acc = num_correct_train / len(train_loader.dataset)\n","print(\"Train accuracy: {:.3f}\".format(train_acc * 100))\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}